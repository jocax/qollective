//! Prompt orchestration and LLM configuration types

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use uuid::Uuid;

use super::enums::*;
use super::internal_api::GenerationRequest;

/// Core structure containing LLM prompts
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptPackage {
    /// LLM system instruction
    pub system_prompt: String,

    /// LLM user message with context
    pub user_prompt: String,

    /// Prompt language
    pub language: Language,

    /// LLM model identifier
    pub llm_model: String,

    /// LLM-specific configuration
    pub llm_config: LLMConfig,

    /// Generation context and audit information
    pub prompt_metadata: PromptMetadata,

    /// True if config.toml fallback was used
    pub fallback_used: bool,
}

/// LLM-specific configuration parameters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LLMConfig {
    /// Randomness (0.0=deterministic, 2.0=very creative)
    #[serde(default = "default_temperature")]
    pub temperature: f32,

    /// Maximum tokens in LLM response
    #[serde(default = "default_max_tokens")]
    pub max_tokens: i32,

    /// Nucleus sampling threshold (0.0-1.0)
    #[serde(default = "default_top_p")]
    pub top_p: f32,

    /// Repetition penalty (-2.0 to 2.0)
    #[serde(default)]
    pub frequency_penalty: f32,

    /// Topic diversity penalty (-2.0 to 2.0)
    #[serde(default)]
    pub presence_penalty: f32,

    /// Token sequences that stop generation
    #[serde(default)]
    pub stop_sequences: Vec<String>,
}

fn default_temperature() -> f32 {
    0.7
}
fn default_max_tokens() -> i32 {
    2000
}
fn default_top_p() -> f32 {
    0.9
}

/// Audit trail and context tracking for prompt generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptMetadata {
    /// ISO 8601 timestamp of prompt generation
    pub generated_at: chrono::DateTime<chrono::Utc>,

    /// Version of prompt template used
    pub template_version: String,

    /// Method used to generate this prompt
    pub generation_method: PromptGenerationMethod,

    /// Age group from GenerationRequest
    pub age_group_context: AgeGroup,

    /// Language from GenerationRequest
    pub language_context: Language,

    /// Which MCP service will use this prompt
    pub service_target: MCPServiceType,

    /// Theme from GenerationRequest (for audit)
    pub theme_context: String,
}

/// Input to prompt-helper MCP tools
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptGenerationRequest {
    /// Complete generation context
    pub generation_request: GenerationRequest,

    /// Which MCP service needs prompts
    pub service_target: MCPServiceType,

    /// Optional: specific node information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub node_context: Option<NodeContext>,

    /// Optional: batch generation context
    #[serde(skip_serializing_if = "Option::is_none")]
    pub batch_info: Option<BatchInfo>,
}

/// Context for node-specific prompt generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeContext {
    /// Unique node identifier
    pub node_id: Uuid,

    /// Position in DAG (1-N)
    pub node_position: i32,

    /// Whether this is a convergence node
    pub is_convergence_point: bool,

    /// Number of edges leading to this node
    pub incoming_edges: i32,

    /// Optional: content from previous node(s)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub previous_content: Option<String>,
}

/// Context for batch generation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BatchInfo {
    /// Unique batch identifier
    pub batch_id: Uuid,

    /// Number of nodes in this batch
    pub batch_size: i32,

    /// Which batch in sequence (0-based)
    pub batch_index: i32,

    /// Total number of batches
    pub total_batches: i32,
}

/// Audit trail of prompt generation for entire generation request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptGenerationSummary {
    /// When prompts were generated
    pub prompts_generated_at: chrono::DateTime<chrono::Utc>,

    /// Time taken to generate all prompts (milliseconds)
    pub prompt_generation_duration_ms: i64,

    /// All prompts generated (keyed by service type)
    pub prompts_used: HashMap<String, PromptPackage>,

    /// Number of prompts using config.toml fallback
    pub fallback_count: i32,

    /// Number of prompts dynamically generated
    pub llm_generated_count: i32,
}

/// Prompt generation statistics
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct PromptGenerationStats {
    /// Total prompts generated
    pub total: i32,

    /// Prompts using fallback
    pub fallback_count: i32,

    /// Prompts generated by LLM
    pub llm_generated: i32,
}
