# Story Generator MCP Server Configuration
# All values can be overridden by environment variables with STORY_GENERATOR_ prefix
# Example: STORY_GENERATOR_NATS_URL=nats://production:4222

[service]
name = "story-generator"
version = "0.1.0"
description = "TaleTrail Story Generator MCP Server - Creates DAG structures and generates narrative content"

[nats]
url = "nats://localhost:5222"
subject = "mcp.story.generate"
queue_group = "story-generator"

[nats.auth]
# NKey authentication (replaces mTLS client certificates)
# Option 1: Use NKey file (development/local)
nkey_file = "../nkeys/story-generator.nk"

# Option 2: Use NKey seed value directly (production - from env var)
# nkey_seed = "SUACGT..."  # Usually set via STORY_GENERATOR_NATS_AUTH_NKEY_SEED env var
# Priority: nkey_seed (highest) > nkey_file (fallback)

[nats.tls]
# TLS for encryption (no client cert needed with NKey auth)
ca_cert = "../certs/ca.pem"

[generation]
timeout_secs = 180
batch_size_min = 2
batch_size_max = 2
target_words_per_node = 400
request_delay_ms = 2000          # Delay between LLM requests (Google rate limiting)
discovery_delay_ms = 100         # Delay for discovery responses

[llm]
type = "lmstudio"
url = "http://127.0.0.1:1234/v1"
default_model = "meta-llama-3-8b-instruct"
use_default_model_fallback = true
timeout_secs = 180
max_tokens = 2048
temperature = 0.7
system_prompt_style = "chatml"

[llm.debug]
dump_raw_response_enabled = true
#dump_directory = "/tmp/taletrail_llm_responses"
dump_directory = "/Users/ms/development/qollective/software/examples/qollective_taletrail_content_generator/desktop/test-trails-llm-responses"
dump_all_responses = false

[llm.models]
en = "meta-llama-3-8b-instruct"
de = "leolm-70b-chat"
