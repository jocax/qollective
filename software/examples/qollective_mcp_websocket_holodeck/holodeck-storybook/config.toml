# Holodeck Storybook Service Configuration

[service]
name = "holodeck-storybook"
version = "0.1.0"

[llm]
# Primary LLM provider: "openai", "ollama", "anthropic", "perplexity"
provider = "ollama"

# Model name to use - optimized for server management and content aggregation
model = "gemma2:2b"

# Optional: Custom API key (overrides .env file)
# api_key = "your_custom_key_here"

# Optional: Custom endpoint URL (for Ollama or custom providers)
# endpoint_url = "http://localhost:11434"

# Response parameters - optimized for server management tasks
temperature = 0.5
max_tokens = 2000
timeout_seconds = 30

[storybook]
# Server configuration
rest_server_port = 8080    # Will be overridden by constants::HOLODECK_STORYBOOK_PORT
websocket_port = 8080      # WebSocket upgrade on same port

# Session management
max_concurrent_sessions = 1000
session_timeout_minutes = 480  # 8 hours

# Content and performance
content_cache_size_mb = 500
real_time_update_interval_ms = 100
story_persistence_enabled = true

# Service integration
content_validation_timeout_seconds = 10