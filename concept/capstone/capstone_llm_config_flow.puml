@startuml
!theme black-knight
skinparam backgroundColor #1a1a1a
skinparam defaultFontColor #e0e0e0
skinparam sequence {
    ArrowColor #4CAF50
    ArrowFontColor #B0BEC5
    LifeLineBorderColor #757575
    LifeLineBackgroundColor #2d2d2d
    ParticipantBorderColor #4CAF50
    ParticipantBackgroundColor #263238
    ParticipantFontColor #FFFFFF
    ActorBorderColor #FF9800
    ActorBackgroundColor #37474F
    ActorFontColor #FFFFFF
    BoxBorderColor #546E7A
    BoxBackgroundColor #1e1e1e
    GroupBorderColor #546E7A
    GroupBackgroundColor #212121
}

title **LLM Configuration Override Flow (4-Layer Priority System)**

participant ".env File" as DotEnv #FF6B6B
participant "config.toml" as TOML #4ECDC4
participant "System ENV" as SysEnv #95E1D3
participant "Figment\nMerge" as Figment #F38181
participant "LlmConfig" as Config #AA96DA
participant "MCP Server\n(prompt-helper)" as MCP1 #8FD9A8
participant "MCP Server\n(story-generator)" as MCP2 #FCBAD3

== Application Startup ==

MCP1 -> DotEnv: Load .env file\n(if exists)
activate DotEnv
DotEnv --> MCP1: LLM_OPENAI_API_KEY=sk-...\nLLM_ANTHROPIC_API_KEY=sk-ant-...\nLLM_GOOGLE_API_KEY=AIza...
deactivate DotEnv

MCP1 -> TOML: Load config.toml
activate TOML
TOML --> MCP1: [llm]\ntype = "lmstudio"\nurl = "http://127.0.0.1:1234/v1"\napi_key = "toml-key"
deactivate TOML

MCP1 -> SysEnv: Check system env vars\n(LLM_* prefix)
activate SysEnv
SysEnv --> MCP1: LLM_TYPE=openai\nLLM_API_KEY=system-key
deactivate SysEnv

== Configuration Merge (Static Layers 1-3: TOML < .env < System ENV) ==

note over DotEnv, SysEnv
**dotenvy Behavior:**
- Loads .env into process environment
- Does NOT override existing system vars
- Therefore: System ENV > .env automatically
end note

MCP1 -> Figment: Merge static layers\n1. TOML\n2. .env\n3. System ENV
activate Figment

note right of Figment
**Static Override Priority (Layers 1-3):**
1. config.toml (lowest - base defaults)
2. .env file (middle - dev config)
3. System ENV (highest - runtime secrets)

**Layer 4 (Optional):**
Request payload TenantLlmConfig
- Applied at runtime per-request
- Overrides ALL static layers
- Used for premium tenants

**Provider-Specific Keys:**
- LLM_OPENAI_API_KEY
- LLM_ANTHROPIC_API_KEY
- LLM_GOOGLE_API_KEY
- LLM_API_KEY (fallback)
end note

Figment -> Figment: Layer 1: Apply TOML\n(base configuration)
Figment -> Figment: Layer 2: Apply .env\n(overrides TOML)
Figment -> Figment: Layer 3: Apply System ENV\n(overrides TOML + .env)

Figment --> MCP1: Merged LlmConfig
deactivate Figment

== Provider Resolution ==

MCP1 -> Config: Create LlmConfig\nwith merged values
activate Config

Config -> Config: Validate configuration\n- Check required fields\n- Validate temperature\n- Verify URLs

note right of Config
**Resolved Static Config (Layers 1-3):**
type = "openai"  (from Layer 3: System ENV)
api_key = "system-key"  (from Layer 3: System ENV)
url = "http://127.0.0.1:1234/v1"  (from Layer 1: TOML)
default_model = "meta-llama-3-8b-instruct"  (from Layer 1: TOML)

**Note:** Layer 4 (runtime request override)
can still override these values per-request
end note

Config --> MCP1: ✅ Valid LlmConfig
deactivate Config

== Same Process for Other MCP Servers ==

MCP2 -> DotEnv: Load .env file
activate DotEnv
DotEnv --> MCP2: Same .env values
deactivate DotEnv

MCP2 -> TOML: Load story-generator/config.toml
activate TOML
TOML --> MCP2: [llm] configuration
deactivate TOML

MCP2 -> SysEnv: Check system env vars
activate SysEnv
SysEnv --> MCP2: STORY_GENERATOR_LLM_*\nor LLM_*
deactivate SysEnv

MCP2 -> Figment: Merge with same priority
activate Figment
Figment --> MCP2: Merged LlmConfig
deactivate Figment

MCP2 -> Config: Create & validate
activate Config
Config --> MPC2: ✅ Valid LlmConfig
deactivate Config

== Runtime Request (Without Layer 4 Override) ==

note over MCP1, MCP2
Both servers now have LlmConfig
with properly overridden API keys
end note

MCP1 -> MCP1: Incoming MCP request\n(No TenantLlmConfig)
MCP1 -> Config: Get provider client\nusing static LlmConfig
activate Config

Config -> Config: Build rig-core client\nwith api_key from static config

Config --> MCP1: DynamicLlmClient\n(using static config)
deactivate Config

MCP1 -> MCP1: Process request\nusing LLM provider

== Runtime Request with Layer 4 Override (Optional - Highest Priority) ==

participant "Orchestrator" as Orch #FFA07A
participant "TenantLlmConfig\n(Request Payload)" as TenantCfg #FFD700

Orch -> MCP1: MCP Request with\n**TenantLlmConfig** in payload
activate TenantCfg

note right of TenantCfg
**Layer 4: Runtime Override (Highest Priority)**
TenantLlmConfig {
  tenant_id: "premium-corp",
  provider_type: OpenAI,
  api_key: "sk-tenant-owned-key",  ⭐
  model_overrides: {
    en: "gpt-4-turbo"
  },
  max_tokens: 8192
}

**This overrides ALL static layers!**
end note

MCP1 -> Config: Check for TenantLlmConfig\nin request payload
activate Config

Config -> Config: TenantLlmConfig found!\nApply Layer 4 override

note right of Config
**Priority Resolution:**
1. TOML: type="shimmy", api_key=None
2. .env: LLM_TYPE="lmstudio"
3. System ENV: LLM_TYPE="openai", api_key="system-key"
4. **Request: provider=OpenAI, api_key="sk-tenant-owned-key"**

**Final Config (Layer 4 wins):**
provider_type = OpenAI  ✅ (from request)
api_key = "sk-tenant-owned-key"  ✅ (from request)
model = "gpt-4-turbo"  ✅ (from request)
max_tokens = 8192  ✅ (from request)
url = "http://127.0.0.1:1234/v1"  (from static)
end note

Config -> Config: Build rig-core client\nwith **tenant's** credentials

Config --> MCP1: DynamicLlmClient\n(using tenant's OpenAI account)
deactivate Config
deactivate TenantCfg

MCP1 -> MCP1: Process request\nusing **tenant's LLM provider**

note over Orch, MCP1
**Use Case: Premium Tenants**
- Tenant provides their own API keys at request time
- No API keys stored in database
- Credentials passed in request payload
- Automatically logged via tracing::info!
- Full tenant isolation
end note

@enduml
